{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Data Preparation\n",
                "\n",
                "This notebook downloads the **ikigai Pothole V2** dataset for YOLOv8/v9/v10 training.\n",
                "\n",
                "**Dataset**: ikigai Pothole V2 (1,481 augmented images, 640√ó640)\n",
                "\n",
                "**Outputs saved to Google Drive**: `/MyDrive/PotholeDetection/`\n",
                "\n",
                "## Prerequisites\n",
                "1. Roboflow account (free tier)\n",
                "2. API key from https://app.roboflow.com/settings/api"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Output Directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "\n",
                "# All outputs will be stored here\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/PotholeDetection'\n",
                "DATASET_DIR = f'{OUTPUT_DIR}/dataset'\n",
                "\n",
                "# Clean up any previous failed downloads\n",
                "if os.path.exists(DATASET_DIR):\n",
                "    print(f'Removing existing dataset directory...')\n",
                "    shutil.rmtree(DATASET_DIR)\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f'Output directory: {OUTPUT_DIR}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download ikigai Pothole V2 Dataset\n",
                "\n",
                "**Dataset Details:**\n",
                "- Source: 617 images ‚Üí Augmented: 1,481 images\n",
                "- Classes: 1 (pothole)\n",
                "- Image size: 640√ó640 (pre-resized for YOLO)\n",
                "- License: CC BY 4.0\n",
                "- Benchmark mAP@50: 75.8%\n",
                "\n",
                "‚ö†Ô∏è **Replace `YOUR_API_KEY` with your actual Roboflow API key**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q roboflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "# ========================================\n",
                "# ‚ö†Ô∏è ENTER YOUR API KEY HERE\n",
                "# ========================================\n",
                "ROBOFLOW_API_KEY = \"AQCMdziIQXHSgs3xmEME\"  # <-- Replace this!\n",
                "\n",
                "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
                "\n",
                "# ikigai Pothole V2 Dataset\n",
                "# - Workspace: ikigai\n",
                "# - Project: pothole-v2-m6ldn\n",
                "# - Version: 22 (with augmentations)\n",
                "project = rf.workspace(\"ikigai\").project(\"pothole-v2-m6ldn\")\n",
                "dataset = project.version(22).download(\"yolov8\", location=DATASET_DIR)\n",
                "\n",
                "print(f'\\n‚úÖ Dataset downloaded to: {dataset.location}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Verify Dataset Structure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "dataset_location = dataset.location\n",
                "\n",
                "print(f\"Dataset location: {dataset_location}\")\n",
                "print(\"\\nDataset Structure Verification\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# List top-level contents\n",
                "print(\"Contents:\", os.listdir(dataset_location))\n",
                "\n",
                "stats = {}\n",
                "for split in ['train', 'valid', 'test']:\n",
                "    # Check multiple possible structures\n",
                "    possible_paths = [\n",
                "        (f'{dataset_location}/{split}/images', f'{dataset_location}/{split}/labels'),\n",
                "        (f'{dataset_location}/images/{split}', f'{dataset_location}/labels/{split}'),\n",
                "        (f'{dataset_location}/{split}', f'{dataset_location}/{split}'),\n",
                "    ]\n",
                "    \n",
                "    found = False\n",
                "    for img_path, lbl_path in possible_paths:\n",
                "        if os.path.exists(img_path):\n",
                "            images = [f for f in os.listdir(img_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
                "            labels = [f for f in os.listdir(lbl_path) if f.endswith('.txt')] if os.path.exists(lbl_path) else []\n",
                "            stats[split] = {'images': len(images), 'labels': len(labels)}\n",
                "            print(f\"{split.upper():6} | Images: {len(images):4} | Labels: {len(labels):4}\")\n",
                "            found = True\n",
                "            break\n",
                "    \n",
                "    if not found:\n",
                "        print(f\"{split.upper():6} | Not found\")\n",
                "\n",
                "print(\"=\" * 40)\n",
                "total_images = sum(s.get('images', 0) for s in stats.values())\n",
                "print(f\"TOTAL  | Images: {total_images}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Dataset Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from PIL import Image\n",
                "import glob\n",
                "\n",
                "# Collect image sizes\n",
                "image_files = glob.glob(f'{dataset_location}/**/*.jpg', recursive=True)\n",
                "image_files += glob.glob(f'{dataset_location}/**/*.jpeg', recursive=True)\n",
                "image_files += glob.glob(f'{dataset_location}/**/*.png', recursive=True)\n",
                "\n",
                "print(f\"Found {len(image_files)} images\")\n",
                "\n",
                "sizes = []\n",
                "for img_path in image_files[:100]:\n",
                "    try:\n",
                "        with Image.open(img_path) as img:\n",
                "            sizes.append(img.size)\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "# Count annotations\n",
                "label_files = glob.glob(f'{dataset_location}/**/*.txt', recursive=True)\n",
                "label_files = [f for f in label_files if 'classes' not in f.lower() and 'readme' not in f.lower()]\n",
                "\n",
                "total_annotations = 0\n",
                "for lbl_path in label_files:\n",
                "    try:\n",
                "        with open(lbl_path, 'r') as f:\n",
                "            lines = [l for l in f.readlines() if l.strip()]\n",
                "            total_annotations += len(lines)\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "# Build stats\n",
                "dataset_stats = {\n",
                "    'name': 'ikigai Pothole V2',\n",
                "    'source': 'Roboflow Universe',\n",
                "    'workspace': 'ikigai',\n",
                "    'project': 'pothole-v2-m6ldn',\n",
                "    'version': 22,\n",
                "    'license': 'CC BY 4.0',\n",
                "    'total_images': len(image_files),\n",
                "    'total_annotations': total_annotations,\n",
                "    'avg_annotations_per_image': round(total_annotations / max(len(image_files), 1), 2),\n",
                "    'splits': stats,\n",
                "    'classes': ['pothole'],\n",
                "    'image_size': '640x640',\n",
                "    'augmentations': ['horizontal_flip', 'brightness_15pct', 'exposure_15pct']\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\" * 40)\n",
                "print(\"DATASET STATISTICS\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Name: {dataset_stats['name']}\")\n",
                "print(f\"Total Images: {dataset_stats['total_images']}\")\n",
                "print(f\"Total Annotations: {dataset_stats['total_annotations']}\")\n",
                "print(f\"Avg Annotations/Image: {dataset_stats['avg_annotations_per_image']}\")\n",
                "print(f\"Classes: {dataset_stats['classes']}\")\n",
                "print(f\"Image Size: {dataset_stats['image_size']}\")\n",
                "print(f\"License: {dataset_stats['license']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Statistics to Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save stats as JSON for DATA_CARD.md\n",
                "stats_path = f'{OUTPUT_DIR}/dataset_stats.json'\n",
                "with open(stats_path, 'w') as f:\n",
                "    json.dump(dataset_stats, f, indent=2, default=str)\n",
                "\n",
                "print(f'‚úÖ Statistics saved to: {stats_path}')\n",
                "print('\\nüìã Copy these stats to your DATA_CARD.md:')\n",
                "print(json.dumps(dataset_stats, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Sample Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "from PIL import Image\n",
                "import random\n",
                "import os\n",
                "\n",
                "def visualize_sample(img_path):\n",
                "    \"\"\"Display image with bounding boxes\"\"\"\n",
                "    img = Image.open(img_path)\n",
                "    w, h = img.size\n",
                "    \n",
                "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
                "    ax.imshow(img)\n",
                "    \n",
                "    # Try to find corresponding label file\n",
                "    base = os.path.splitext(img_path)[0]\n",
                "    possible_labels = [\n",
                "        base + '.txt',\n",
                "        img_path.replace('/images/', '/labels/').replace('.jpg', '.txt').replace('.png', '.txt')\n",
                "    ]\n",
                "    \n",
                "    for label_path in possible_labels:\n",
                "        if os.path.exists(label_path):\n",
                "            with open(label_path, 'r') as f:\n",
                "                for line in f:\n",
                "                    parts = line.strip().split()\n",
                "                    if len(parts) >= 5:\n",
                "                        _, x_center, y_center, width, height = map(float, parts[:5])\n",
                "                        x1 = (x_center - width/2) * w\n",
                "                        y1 = (y_center - height/2) * h\n",
                "                        box_w = width * w\n",
                "                        box_h = height * h\n",
                "                        \n",
                "                        rect = patches.Rectangle((x1, y1), box_w, box_h, \n",
                "                                                linewidth=2, edgecolor='red', facecolor='none')\n",
                "                        ax.add_patch(rect)\n",
                "            break\n",
                "    \n",
                "    ax.axis('off')\n",
                "    plt.title(os.path.basename(img_path))\n",
                "    plt.show()\n",
                "\n",
                "# Display 3 random samples\n",
                "if image_files:\n",
                "    sample_images = random.sample(image_files, min(3, len(image_files)))\n",
                "    for img_path in sample_images:\n",
                "        visualize_sample(img_path)\n",
                "else:\n",
                "    print(\"No images found to visualize\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Phase 1 Complete!\n",
                "\n",
                "**What's saved to Google Drive:**\n",
                "- Dataset: `/MyDrive/PotholeDetection/dataset/`\n",
                "- Stats: `/MyDrive/PotholeDetection/dataset_stats.json`\n",
                "\n",
                "**Dataset Summary:**\n",
                "- ikigai Pothole V2 (version 22)\n",
                "- ~1,481 augmented images (640√ó640)\n",
                "- Ready for YOLOv8/v9/v10/v11 training\n",
                "\n",
                "**Next Step:** Proceed to Phase 2 (Model Training) using `model_comparison.ipynb`"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
